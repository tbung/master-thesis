\chapter{Theoretical Background}%
\label{cha:background}

This chapter will give an overview of the theoretical background of the problem
investigated in this thesis, as well as the methods used. We will first
introduce the topic of outliers on an intuitive level and then try to highlight
a more rigorous approach. We will move on to introduce neural network models
especially in their invertible variant and how specific training schemes are
particularly useful for investigating outliers. Finally, we will introduce
archetypal analysis as a general concept and also as an add-on to more
complicated models.


\section{Outliers And Outlier Detection}%
\label{sec:outliers_an_outlier_detection}

Outliers are data points in a data set, that differ significantly from other
data points in the dataset in a way that makes them not indicative of other
data from the same source. It is therefore necessary to determine what data
points are outliers and how they should be handled in any task performed on the
dataset~\citep{europeanmathematicalsocietyOutlierEncyclopediaMathematics}.
Especially for extreme data points that still somewhat resemble the rest of the
dataset it can be difficult to decide whether or not they are an outlier and
this decision might ultimately subjective~\citep{zimekThereBackAgain2018}.
% TODO: Expand


\section{Invertible Neural Networks}% Generative Models?
\label{sec:invertible_neural_networks}

% - Neural Networks and their importance (briefly)
Neural networks and deep learning methods have become staple methods in many
fields that use machine learning. They are typically comprised of a chain of relatively
simple, non-linear blocks.~\citep{lecunDeepLearning2015}

As representation-learning methods, neural networks have found use for
many discriminative as well as generative tasks. Especially in recent years
many advances were made for generative methods using neural networks. One such
advancement was the introduction of invertible neural networks. Neural
networks, though not invertible in general, can be made fully invertible by
cleverly designing its building blocks to be invertible.
% invertible -> bijective?

% We will now briefly introduce two other generative models based on neural networks
% before introducing invertible neural networks in detail.

% \paragraph{Generative Adversarial Networks}%
% \label{par:generative_adversarial_networks}

% \paragraph{Variational Autoencoders}%
% \label{par:variational_autoencoders}

\subsection{Coupling Layers}%
\label{sub:coupling_layers}

In general, neural networks are composed of multiple layers or composite
functions. The function $\mathcal{F}$ a neural network with $L$ layers computes
is given by
\begin{equation}%
    \label{eq:neural_network}
    \mathcal{F} = f_L \circ \dots \circ f_2 \circ f_1\;.
\end{equation}

If we design all layers to be bijective and have tractable Jacobian
determinants the whole forward computation is invertible by composing the
inverse layers in reverse order, i.e.
\begin{equation}%
    \label{eq:neural_network_inv}
    \mathcal{F}^{-1} = f_L^{-1} \circ \dots \circ f_2^{-1} \circ f_1^{-1}\;.
\end{equation}

Dinh et al.\ \citep{dinhNICENonlinearIndependent2015} describe a family of transformations
called coupling layers, that can be used as components in a invertible neural
network. In general, coupling layers are defined by Dinh et al. as follows

\begin{definition}[General Coupling Layer]
    Let $\mathbf{x} \in \mathbb{R}^D$
    and $I_1, I_2$ a partition of $[1, D]$ such that $d = |I_1|$ and $m$ a
    function defined on $\mathbb{R}^d$. We can then define $\mathbf{y} =
    [\mathbf{y}_{I_1}, \mathbf{y}_{I_2}]$ as
    \begin{equation}
        \begin{aligned}%
            \label{eq:coupling_layer_transform}
            \mathbf{y}_{I_1} &= \mathbf{x}_{I_1} \\
            \mathbf{y}_{I_2} &= g(\mathbf{x}_{I_2};m(\mathbf{x}_{I_1}))\;,
        \end{aligned}
    \end{equation}
    where $g: \mathbb{R}^{D-d} \times m(\mathbb{R}^d) \rightarrow
    \mathbb{R}^{D-d}$ is the coupling law, an invertible map with respect to
    the first argument given its second. This transformation is called coupling
    layer with coupling function $m$.
\end{definition}

Note that the choice of $m$ is not constrained to invertible functions but can
be arbitrary.

% TODO: Additive coupling layer, volume preserving %
A simple choice for the coupling law would be an additive law
~\citep{dinhNICENonlinearIndependent2015, gomezReversibleResidualNetwork2017}.
The coupling layer transformation then becomes
\begin{equation}
    \begin{aligned}%
        \label{eq:add_coupling_transform}
        \mathbf{y}_{I_1} &= \mathbf{x}_{I_1} \\
        \mathbf{y}_{I_2} &= \mathbf{x}_{I_2} + m(\mathbf{x}_{I_1})\;.
    \end{aligned}
\end{equation}
This transformation has unit Jacobian determinant, i.e.\ is volume preserving.
This can give numerical stability especially if $m$ is a rectified linear
network. This however comes at the cost of what transformations can be learned.

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \begin{tikzpicture}
            \node[varnode] (x1)                   {$\mathbf{x}_{I_1}$};
            \node[connectnode] (c1)   [right=0.5cm of x1]    {};
            \node[varnode] (y1) [right=5cm of x1] {$\mathbf{y}_{I_1}$};
            \node[varnode] (x2) [below=2.5cm of x1] {$\mathbf{x}_{I_2}$};
            \node[varnode] (y2) [right=5cm of x2] {$\mathbf{y}_{I_2}$};
            \node[opnode] (plus) [right=3.0cm of x2] {$+$};
            \node[opnode] (m) at ($(c1)!2cm!(plus)$) {$m$};


            \draw[grapharrow] (x1) -- (y1);
            \draw[grapharrow] (c1) -- (m);
            \draw[grapharrow] (x2) -- (plus);
            \draw[grapharrow] (m) -- (plus);
            \draw[grapharrow] (plus) -- (y2);
        \end{tikzpicture}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \begin{tikzpicture}
            \node[varnode] (x1)                   {$\mathbf{x}_{I_1}$};
            \node[varnode] (y1) [right=5cm of x1] {$\mathbf{y}_{I_1}$};
            \node[connectnode] (c1)   [left=0.5cm of y1]    {};
            \node[varnode] (x2) [below=2.5cm of x1] {$\mathbf{x}_{I_2}$};
            \node[varnode] (y2) [right=5cm of x2] {$\mathbf{y}_{I_2}$};
            \node[opnode] (plus) [left=3.0cm of y2] {$-$};
            \node[opnode] (m) at ($(c1)!2cm!(plus)$) {$m$};


            \draw[grapharrow] (y1) -- (x1);
            \draw[grapharrow] (c1) -- (m);
            \draw[grapharrow] (y2) -- (plus);
            \draw[grapharrow] (m) -- (plus);
            \draw[grapharrow] (plus) -- (x2);
        \end{tikzpicture}
    \end{subfigure}
    \caption{Additive coupling block. \textbf{Top}: forward computation,
        \textbf{Bottom}: inverse computation.}%
\label{fig:additive_coupling_block}
\end{figure}

% TODO: GLOW coupling blocks %
A more versatile approach is to use affine coupling blocks, that allow for
non-volume preserving transformations. The coupling layer transformation for
the general affine coupling block introduced by Dinh et
al.~\citep{dinhDensityEstimationUsing2017} is
\begin{equation}
    \begin{aligned}%
        \label{eq:affine_coupling_transform}
        \mathbf{y}_{I_1} &= \mathbf{x}_{I_1} \\
        \mathbf{y}_{I_2} &= \mathbf{x}_{I_2} \odot \exp ( s(\mathbf{x}_{I_1}))
        + t(\mathbf{x}_{I_1})\;.
    \end{aligned}
\end{equation}
where $s$ and $t$ are arbitrary functions that map $\mathbb{R}^{D-d}
\rightarrow \mathbb{R}^d$ and stand for scale and translation respectively and
$\odot$ is the Hadarmat or pointwise product.
This transformation is again easily invertible, regardless of the complexity of
$s$ and $t$
\begin{equation}
    \begin{aligned}%
        \label{eq:inv_affine_coupling_transform}
        \mathbf{x}_{I_1} &= \mathbf{y}_{I_1} \\
        \mathbf{x}_{I_2} &= (\mathbf{y}_{I_2} - t(\mathbf{y}_{I_1})) \odot \exp
        (-s(\mathbf{y}_{I_1}))\;.
    \end{aligned}
\end{equation}

\begin{figure}[htpb]
\begin{center}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \begin{tikzpicture}
            \node[varnode]     (x1)                          {$\mathbf{x}_{I_1}$};
            \node[connectnode] (c1)   [right=0.5cm of x1]    {};
            \node[connectnode] (c2)   [right=1.7cm of x1]    {};
            \node[varnode]     (y1)   [right=5cm   of x1]    {$\mathbf{y}_{I_1}$};
            \node[varnode]     (x2)   [below=2.5cm of x1]    {$\mathbf{x}_{I_2}$};
            \node[varnode]     (y2)   [right=5cm   of x2]    {$\mathbf{y}_{I_2}$};
            \node[opnode]      (plus) [right=1.5cm of x2]    {$+$};
            \node[opnode]      (mul)  [right=2.7cm of x2]    {$\times$};
            \node[opnode]      (t)    at ($(c1)!2cm!(plus)$) {$t$};
            \node[opnode]      (s)    at ($(c2)!2cm!(mul)$)  {$s$};


            \draw[grapharrow] (x1)   -- (y1);
            \draw[grapharrow] (c1)   -- (t);
            \draw[grapharrow] (c2)   -- (s);
            \draw[grapharrow] (x2)   -- (plus);
            \draw[grapharrow] (t)    -- (plus);
            \draw[grapharrow] (s)    -- (mul);
            \draw[grapharrow] (plus) -- (mul);
            \draw[grapharrow] (mul)  -- (y2);
        \end{tikzpicture}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \begin{tikzpicture}
            \node[varnode]     (x1)                          {$\mathbf{x}_{I_1}$};
            \node[varnode]     (y1)   [right=5cm   of x1]    {$\mathbf{y}_{I_1}$};
            \node[connectnode] (c1)   [left=0.5cm of y1]    {};
            \node[connectnode] (c2)   [left=1.7cm of y1]    {};
            \node[varnode]     (x2)   [below=2.5cm of x1]    {$\mathbf{x}_{I_2}$};
            \node[varnode]     (y2)   [right=5cm   of x2]    {$\mathbf{y}_{I_2}$};
            \node[opnode]      (plus) [left=1.5cm of y2]    {$-$};
            \node[opnode]      (mul)  [left=2.7cm of y2]    {$\div$};
            \node[opnode]      (t)    at ($(c1)!2cm!(plus)$) {$t$};
            \node[opnode]      (s)    at ($(c2)!2cm!(mul)$)  {$s$};


            \draw[grapharrow] (y1)   -- (x1);
            \draw[grapharrow] (c1)   -- (t);
            \draw[grapharrow] (c2)   -- (s);
            \draw[grapharrow] (y2)   -- (plus);
            \draw[grapharrow] (t)    -- (plus);
            \draw[grapharrow] (s)    -- (mul);
            \draw[grapharrow] (plus) -- (mul);
            \draw[grapharrow] (mul)  -- (x2);
        \end{tikzpicture}
    \end{subfigure}
\end{center}
    \caption{Affine coupling block. \textbf{Top}: forward computation,
        \textbf{Bottom}: inverse computation.}%
\label{fig:affine_coupling_block}
\end{figure}

To ensure all inputs are used we need to use at least two coupling blocks and
switch $\mathbf{x}_{I_1}$ and $\mathbf{x}_{I_2}$ between them.

% TODO: IRevNetDownsampling and permute layers %
\subsection{Downsampling}%
\label{sub:downsampling}

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}
    \foreach \x in {0,...,2}
    \foreach \y in {0,...,2}
    {
        \node [square,draw,fill=gray!40,minimum size=1cm] (\x\y) at (2cm*\x,2cm*\y) {};
        \node [square,draw,fill=red!40,minimum size=1cm] (\x\y) at (1cm+2cm*\x,2cm*\y) {};
        \node [square,draw,fill=blue!40,minimum size=1cm] (\x\y) at (2cm*\x,1cm+2cm*\y) {};
        \node [square,draw,fill=green!40,minimum size=1cm] (\x\y) at (1cm+2cm*\x,1cm+2cm*\y) {};
    }

    \foreach \x in {0,...,2}
    \foreach \y in {0,...,2}
    {
        \node [square,draw,fill=gray!40,minimum size=1cm] (\x\y) at
            (10cm+1cm*\x,2cm+1cm*\y) {};
    }

    \foreach \x in {0,...,2}
    \foreach \y in {0,...,2}
    {
        \node [square,draw,fill=red!40,minimum size=1cm] (\x\y) at
            (9.66cm+1cm*\x,1.66cm+1cm*\y) {};
    }

    \foreach \x in {0,...,2}
    \foreach \y in {0,...,2}
    {
        \node [square,draw,fill=blue!40,minimum size=1cm] (\x\y) at
            (9.33cm+1cm*\x,1.33cm+1cm*\y) {};
    }

    \foreach \x in {0,...,2}
    \foreach \y in {0,...,2}
    {
        \node [square,draw,fill=green!40,minimum size=1cm] (\x\y) at
            (9cm+1cm*\x,1cm+1cm*\y) {};
    }

    \draw[{Straight Barb}-{Straight Barb},line width=1mm] (6cm,2.5cm) -- (8cm,2.5cm);
\end{tikzpicture}
\end{center}
\caption{IRevNet Downsampling, figure adapted from Gomez et
al.~\citep{gomezReversibleResidualNetwork2017}. Images are split according to a
checkerboard pattern in the spatial dimensions and rearranged along the channel
dimension.}%
\label{fig:downsampling}
\end{figure}

In typical deep learning architectures, especially in the field of computer
vision, the inputs are downsampled in the network. This is usually done with
strided convolutions, maximum or average pooling. Since these operations entail
a loss of information they are not bijective and cannot be used in an
invertible neural network architecture, at least outside of coupling functions.
To still be able to increase the field of vision of convolutions and decrease the
amount of computations in subsequent layers different strategies can be used. Gomez
et al. used a pixel shuffle operation, in which spatial dimensions are reduce and
shifted into channel dimensions as shown
in~\autoref{fig:downsampling}~\citep{gomezReversibleResidualNetwork2017}. 

A strategy to reduce the amount of computations done in deeper layers, data can
simply be split of. The part that is split of can than be trained to resemble
noise while the other part continues through coupling layers. Towards the end
of the network the split parts are concatenated back together.


\subsection{Normalizing Flows}%
\label{sub:normalizing_flows}

% TODO: Figure latent space x -> z
\begin{figure}[htpb]
    \centering
    \includegraphics{figures/toy_example/gaussian_mixture/grid_transformed.pdf}
    \caption{Mapping between data and latent space of a normalizing flow from a
    mixture of three Gaussians to a single normal distribution. The
transformation of grid lines is also shown in both directions. Figure adapted
from Dinh et al.~\citep{dinhDensityEstimationUsing2017}}%
    \label{fig:normalizing_flow_mapping}
\end{figure}

Normalizing flows transform a simple probability distributions, such as a
standard normal distribution, into a complex, often intractable probability
distribution~\citep{kobyzevNormalizingFlowsIntroduction2020}. They are composed
of a series of invertible transformations, e.g., the coupling layers we defined
above. The complex probability distribution can be evaluated on a sample by
transforming it back to the simple distribution and using the change of
variables formula.

In more mathematical terms, consider vectors $\mathbf{z}_i \in \mathbb{R}^d$
that are distributed according to a simple distribution $p_{\mathbf{z}}:
\mathbb{R}^d \rightarrow \mathbb{R}$. Let $\mathcal{F}: \mathbb{R}^d
\rightarrow \mathbb{R}^d$ be an invertible transformation (e.g. an invertible
neural network). The probability density function of a variable $\mathbf{x}_i =
\mathcal{F} (\mathbf{z}_i)$ can then be computed according to the change of
variables formula
\begin{equation}%
    \label{eq:change_of_variables}
    p_{\mathbf{x}}(\mathbf{x}_i) =
    p_{\mathbf{z}}(\mathcal{F}^{-1}(\mathbf{x}_i)) \lvert J_{\mathbf{x}}
    \rvert^{-1}\;,
\end{equation}
where
\begin{equation}%
    \label{eq:jacobian_det}
    J_{\mathbf{x}} = \det \frac{\partial \mathcal{F}^{-1} (\mathbf{x})}{\partial \mathbf{x}}
\end{equation}
is the Jacobian determinant of the transformation. Particularly, if
$\mathcal{F}$ is defined as in~\autoref{eq:neural_network}, the Jacobian is
simply
\begin{equation}%
    \label{eq:jacobian_chain_rule}
    J_{\mathbf{x}} = \prod \det \frac{\partial f_i^{-1} (\mathbf{x})}{\partial
    \mathbf{x}}\;.
\end{equation}

To train a neural network as a normalizing flow we can simply maximize the
log-likelihood of data samples, or equivalently minimize the negative
log-likelihood. If $p_{\mathbf{z}}$ is the standard normal
distribution, the negative log-likelihood loss for a mini-batch of size $N$ can
be simply calculated as
% TODO: Check this again
\begin{equation}%
    \label{eq:nll_loss}
    \mathrm{NLL} = \frac{1}{N} \sum_i \lVert \mathcal{F}^{-1} (\mathbf{x}_i)
    \rVert^2 - \log J_{\mathbf{x}}\;.
\end{equation}


\section{Archetypal Analysis}%
\label{sec:archetypal_analysis}

% - classical AA
% - deep AA
A common way to represent data by template examples is by using prototypes,
especially in machine learning. Prototypes are typical examples of their class,
that ideally entail all properties of their class. An alternative way, first
proposed by Cutler et al., is to represent data samples as a mixture of
individuals of pure type or archetypes~\citep{cutlerArchetypalAnalysis1994}. A
simple way to formulate this representation is by a variant of principal
components. A data vector $\mathbf{x}_i \in \mathbb{R}^d$ from a dataset of $n$
samples is represented by coefficients $a_{ij}$ and $m \leq \min(n, d)$
archetype vectors $\mathbf{z}_j \in \mathbb{R}^d$, i.e.,
\begin{equation}%
    \label{eq:data_from_archetypes}
    \mathbf{x}_i = \sum_j a_{ij} \mathbf{z}_j\;.
\end{equation}
In turn, the archetypes are computed from the data using coefficients $b_{ji}$
according to
\begin{equation}%
    \label{eq:archetypes_from_data}
    \mathbf{z}_j = \sum_i b_{ji} \mathbf{x}_i\;.
\end{equation}
Without further constraints this approach does not fulfill the requirements,
that the templates are pure archetypes and that $\mathbf{x}_i$ are represented as
mixtures of the archetypes. If we subject the coefficient to the following
constraints:
\begin{equation}
    \begin{aligned}%
        \label{eq:archetype_constraints}
        a_{ij} \geq 0 &\textrm{ and } \sum_{j=1}^{m} a_{ij} = 1\;, \\
        b_{ji} \geq 0 &\textrm{ and } \sum_{i=1}^{n} b_{ji} = 1\;,
    \end{aligned}   
\end{equation}
we obtain the desired representation. The archetypes now represent the data's
convex hull.

Finding the weights $a_{ij}$ and $b_{ij}$ can be done by minimizing the square
error in the representation
\begin{equation}
    % \begin{aligned}
        \label{eq:archetype_rss}
        \min_{a,b} \sum_i \lVert \mathbf{x}_i - \sum_j a_{ij} \mathbf{z}_j
        \rVert^2
        = \min_{a,b} \sum_l \lVert \mathbf{x}_l - \sum_j a_{lj} \sum_i
        b_{ji} \mathbf{x}_i \rVert^2
    % \end{aligned}
\end{equation}
Cutler et al. propose an alternating least squares
algorithm to solve this optimization~\citep{cutlerArchetypalAnalysis1994}.
Bauckhage et al. introduced a more efficient algorithm based on
sub-gradients~\citep{bauckhageArchetypalAnalysisAutoencoder2015}, which is also
used in \textcolor{red}{FIGURE}. % TODO: do I need this?
Keller et al. build on this framework by applying archetypal analysis to the
latent space of a deep generative
model~\citep{kellerLearningExtremalRepresentations2020}. In their paper they
propose using a Deep Variational Information Bottelneck. Here we will use a
normalizing flow instead. The normalizing flow maps data vector $\mathbf{x}_i
\in \mathbb{R}^d$ to a latent vector $\mathbf{t}_i \in \mathbb{R}^d$ that is
approximately normally distributed. Additional linear layers with softmax
activations map the latent vectors to archetype coefficients. As there is no
absolute frame of reference for the latent space coordinates, the $k$ archetype
vectors are initialized to be the vertices of a $(k-1)$-simplex. The archetype
vectors can be fixed when training the flow, or can be allowed to update to
accomodate a pretrained flow. The full model will be discussed in the next
chapter.

% TODO: reformulate with A and B?
