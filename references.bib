
@inproceedings{abrolGeometricApproachArchetypal2020,
  title = {A {{Geometric Approach}} to {{Archetypal Analysis}} via {{Sparse Projections}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Abrol, Vinayak and Sharma, Pulkit},
  date = {2020-11-21},
  pages = {42--51},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v119/abrol20a.html},
  urldate = {2021-04-08},
  abstract = {Archetypal analysis (AA) aims to extract patterns using self-expressive decomposition of data as convex combinations of extremal points (on the convex hull) of the data. This work presents a comput...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  file = {/home/tillb/Zotero/storage/HEKQJ9YA/Abrol and Sharma - 2020 - A Geometric Approach to Archetypal Analysis via Sp.pdf;/home/tillb/Zotero/storage/D95EV9F2/abrol20a.html},
  langid = {english}
}

@incollection{anejaModelingStylizedCharacter2017a,
  title = {Modeling {{Stylized Character Expressions}} via {{Deep Learning}}},
  booktitle = {Computer {{Vision}} – {{ACCV}} 2016},
  author = {Aneja, Deepali and Colburn, Alex and Faigin, Gary and Shapiro, Linda and Mones, Barbara},
  editor = {Lai, Shang-Hong and Lepetit, Vincent and Nishino, Ko and Sato, Yoichi},
  date = {2017},
  volume = {10112},
  pages = {136--153},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-54184-6_9},
  url = {http://link.springer.com/10.1007/978-3-319-54184-6_9},
  urldate = {2021-04-19},
  abstract = {We propose DeepExpr, a novel expression transfer approach from humans to multiple stylized characters. We first train two Convolutional Neural Networks to recognize the expression of humans and stylized characters independently. Then we utilize a transfer learning technique to learn the mapping from humans to characters to create a shared embedding feature space. This embedding also allows human expression-based image retrieval and character expression-based image retrieval. We use our perceptual model to retrieve character expressions corresponding to humans. We evaluate our method on a set of retrieval tasks on our collected stylized character dataset of expressions. We also show that the ranking order predicted by the proposed features is highly correlated with the ranking order provided by a facial expression expert and Mechanical Turk experiments.},
  file = {/home/tillb/Zotero/storage/E9J8GT9T/Aneja et al. - 2017 - Modeling Stylized Character Expressions via Deep L.pdf},
  isbn = {978-3-319-54183-9 978-3-319-54184-6},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{ardizzoneAnalyzingInverseProblems2018,
  title = {Analyzing {{Inverse Problems}} with {{Invertible Neural Networks}}},
  author = {Ardizzone, Lynton and Kruse, Jakob and Rother, Carsten and Köthe, Ullrich},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=rJed6j0cKX},
  urldate = {2021-03-16},
  abstract = {To analyze inverse problems with Invertible Neural Networks},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  file = {/home/tillb/Zotero/storage/WRAK38WS/Ardizzone et al. - 2018 - Analyzing Inverse Problems with Invertible Neural .pdf;/home/tillb/Zotero/storage/FCGI9BG9/forum.html},
  langid = {english}
}

@article{bauckhageArchetypalAnalysisAutoencoder2015,
  title = {Archetypal {{Analysis}} as an {{Autoencoder}}},
  author = {Bauckhage, C. and Kersting, K. and Hoppe, F. and Thurau, C.},
  date = {2015},
  journaltitle = {Workshop New Challenges in Neural Computation 2015},
  pages = {8--16},
  file = {/home/tillb/Zotero/storage/LW24EEIC/Hammer et al. - 2015 - Workshop New Challenges in Neural Computation 2015.pdf},
  langid = {english}
}

@inproceedings{baumSupervisedLearningProbability1988,
  title = {Supervised {{Learning}} of {{Probability Distributions}} by {{Neural Networks}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Baum, Eric and Wilczek, Frank},
  editor = {Anderson, D.},
  date = {1988},
  publisher = {{American Institute of Physics}},
  url = {https://proceedings.neurips.cc/paper/1987/file/eccbc87e4b5ce2fe28308fd9f2a7baf3-Paper.pdf},
  urldate = {2021-05-22},
  file = {/home/tillb/Zotero/storage/TWHK7AYC/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html}
}

@online{cohenEMNISTExtensionMNIST2017,
  title = {{{EMNIST}}: An Extension of {{MNIST}} to Handwritten Letters},
  shorttitle = {{{EMNIST}}},
  author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, André},
  date = {2017-03-01},
  url = {http://arxiv.org/abs/1702.05373},
  urldate = {2021-04-19},
  abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits.},
  archiveprefix = {arXiv},
  eprint = {1702.05373},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/C24TW97H/Cohen et al. - 2017 - EMNIST an extension of MNIST to handwritten lette.pdf;/home/tillb/Zotero/storage/6ZC46HCJ/1702.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  options = {useprefix=true},
  primaryclass = {cs}
}

@book{colesIntroductionStatisticalModeling2001,
  title = {An {{Introduction}} to {{Statistical Modeling}} of {{Extreme Values}}},
  author = {Coles, Stuart},
  date = {2001},
  publisher = {{Springer-Verlag}},
  location = {{London}},
  doi = {10.1007/978-1-4471-3675-0},
  url = {https://www.springer.com/de/book/9781852334598},
  urldate = {2021-06-18},
  abstract = {Directly oriented towards real practical application, this book develops both the basic theoretical framework of extreme value models and the statistical inferential techniques for using these models in practice. Intended for statisticians and non-statisticians alike, the theoretical treatment is elementary, with heuristics often replacing detailed mathematical proof. Most aspects of extreme modeling techniques are covered, including historical techniques (still widely used) and contemporary techniques based on point process models. A wide range of worked examples, using genuine datasets, illustrate the various modeling procedures and a concluding chapter provides a brief introduction to a number of more advanced topics, including Bayesian inference and spatial extremes. All the computations are carried out using S-PLUS, and the corresponding datasets and functions are available via the Internet for readers to recreate examples for themselves. An essential reference for students and researchers in statistics and disciplines such as engineering, finance and environmental science, this book will also appeal to practitioners looking for practical help in solving real problems. Stuart Coles is Reader in Statistics at the University of Bristol, UK, having previously lectured at the universities of Nottingham and Lancaster. In 1992 he was the first recipient of the Royal Statistical Society's research prize. He has published widely in the statistical literature, principally in the area of extreme value modeling.},
  file = {/home/tillb/Zotero/storage/NGJBGD2M/Coles - 2001 - An Introduction to Statistical Modeling of Extreme.pdf;/home/tillb/Zotero/storage/48DBL9QN/9781852334598.html},
  isbn = {978-1-85233-459-8},
  langid = {english},
  series = {Springer {{Series}} in {{Statistics}}}
}

@article{cutlerArchetypalAnalysis1994,
  title = {Archetypal {{Analysis}}},
  author = {Cutler, Adele and Breiman, Leo},
  date = {1994-11-01},
  journaltitle = {Technometrics},
  volume = {36},
  pages = {338--347},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1994.10485840},
  url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1994.10485840},
  urldate = {2021-06-21},
  abstract = {Archetypal analysis represents each individual in a data set as a mixture of individuals of pure type or archetypes. The archetypes themselves are restricted to being mixtures of the individuals in the data set. Archetypes are selected by minimizing the squared error in representing each individual as a mixture of archetypes. The usefulness of archetypal analysis is illustrated on several data sets. Computing the archetypes is a nonlinear least squares problem, which is solved using an alternating minimizing algorithm.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1994.10485840},
  file = {/home/tillb/Zotero/storage/PW4MARU3/Cutler and Breiman - 1994 - Archetypal Analysis.pdf;/home/tillb/Zotero/storage/GW2N7DJ8/00401706.1994.html},
  keywords = {Archetypes,Convex hull,Graphics,Nonlinear optimization,Principal components},
  number = {4}
}

@online{dalalAutoregressiveModelsWhat2019,
  title = {Autoregressive {{Models}}: {{What Are They Good For}}?},
  shorttitle = {Autoregressive {{Models}}},
  author = {Dalal, Murtaza and Li, Alexander C. and Taori, Rohan},
  date = {2019-10-17},
  url = {http://arxiv.org/abs/1910.07737},
  urldate = {2021-04-08},
  abstract = {Autoregressive (AR) models have become a popular tool for unsupervised learning, achieving state-of-the-art log likelihood estimates. We investigate the use of AR models as density estimators in two settings -- as a learning signal for image translation, and as an outlier detector -- and find that these density estimates are much less reliable than previously thought. We examine the underlying optimization issues from both an empirical and theoretical perspective, and provide a toy example that illustrates the problem. Overwhelmingly, we find that density estimates do not correlate with perceptual quality and are unhelpful for downstream tasks.},
  archiveprefix = {arXiv},
  eprint = {1910.07737},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/EHTDCS9B/Dalal et al. - 2019 - Autoregressive Models What Are They Good For.pdf;/home/tillb/Zotero/storage/Y4LFKTDC/1910.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{dinhDensityEstimationUsing2017,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  date = {2017-02-27},
  url = {http://arxiv.org/abs/1605.08803},
  urldate = {2021-04-04},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  archiveprefix = {arXiv},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/DKVDJU8L/Dinh et al. - 2017 - Density estimation using Real NVP.pdf;/home/tillb/Zotero/storage/R8FRYFNP/1605.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{dinhNICENonlinearIndependent2015,
  title = {{{NICE}}: {{Non}}-Linear {{Independent Components Estimation}}},
  shorttitle = {{{NICE}}},
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  date = {2015-04-10},
  url = {http://arxiv.org/abs/1410.8516},
  urldate = {2021-05-22},
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  archiveprefix = {arXiv},
  eprint = {1410.8516},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/RV47GJZD/Dinh et al. - 2015 - NICE Non-linear Independent Components Estimation.pdf;/home/tillb/Zotero/storage/M58U99EF/1410.html},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@incollection{forbesChiSquaredDistribution2010,
  title = {Chi-{{Squared Distribution}}},
  booktitle = {Statistical {{Distributions}}},
  author = {Forbes, Catherine and Evans, Merran and Hastings, Nicholas and Peacock, Brian},
  date = {2010},
  pages = {69--73},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470627242.ch11},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470627242.ch11},
  urldate = {2021-06-18},
  abstract = {Important applications of the chi-squared variate arise from the fact that it is the distribution of the sum of the squares of a number of normal variates. Where a set of data is represented by a theoretical model, the chi-squared distribution can be used to test the goodness of fit between the observed data points and the values predicted by the model, subject to the differences being normally distributed. A particularly common application is the analysis of contingency tables. This chapter discusses variate relationships, random number generation and chi distribution. Controlled Vocabulary Terms chi-squared distribution; control variate},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470627242.ch11},
  file = {/home/tillb/Zotero/storage/MRGQUE62/Forbes et al. - 2010 - Chi-Squared Distribution.pdf;/home/tillb/Zotero/storage/SQMKV25P/9780470627242.html},
  isbn = {978-0-470-62724-2},
  keywords = {chi-squared distribution,random number,variate},
  langid = {english}
}

@online{gomezReversibleResidualNetwork2017,
  title = {The {{Reversible Residual Network}}: {{Backpropagation Without Storing Activations}}},
  shorttitle = {The {{Reversible Residual Network}}},
  author = {Gomez, Aidan N. and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B.},
  date = {2017-07-13},
  url = {http://arxiv.org/abs/1707.04585},
  urldate = {2021-05-28},
  abstract = {Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth.},
  archiveprefix = {arXiv},
  eprint = {1707.04585},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/PQQ5P4ZQ/Gomez et al. - 2017 - The Reversible Residual Network Backpropagation W.pdf;/home/tillb/Zotero/storage/WIJUTPK2/1707.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@book{goochEncyclopedicDictionaryPolymers2011,
  title = {Encyclopedic {{Dictionary}} of {{Polymers}}},
  editor = {Gooch, Jan W.},
  date = {2011},
  edition = {2},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  url = {https://www.springer.com/gp/book/9781441962461},
  urldate = {2021-06-18},
  abstract = {This reference, in its second edition, contains more than 7,500 polymeric material terms, including the names of chemicals, processes, formulae, and analytical methods that are used frequently in the polymer and engineering fields. In view of the evolving partnership between physical and life sciences, this title includes an appendix of biochemical and microbiological terms (thus offering previously unpublished material, distinct from all competitors.) Each succinct entry offers a broadly accessible definition as well as cross-references to related terms. Where appropriate to enhance clarity further, the volume's definitions may also offer equations, chemical structures, and other figures. The new interactive software facilitates easy access to a large database of chemical structures (2D/3D-view), audio files for pronunciation, polymer science equations and many more.},
  file = {/home/tillb/Zotero/storage/KLBRZKVA/9781441962461.html},
  isbn = {978-1-4419-6246-1},
  langid = {english}
}

@inproceedings{goodfellowGenerativeAdversarialNets2014,
  title = {Generative Adversarial Nets},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-12-08},
  pages = {2672--2680},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  file = {/home/tillb/Zotero/storage/8DZVUSLT/Goodfellow et al. - Generative Adversarial Nets.pdf},
  series = {{{NIPS}}'14}
}

@article{haanSampleExtremesElementary1976,
  title = {Sample Extremes: An Elementary Introduction},
  shorttitle = {Sample Extremes},
  author = {de Haan, Laurens},
  date = {1976},
  journaltitle = {Statistica Neerlandica},
  volume = {30},
  pages = {161--172},
  issn = {1467-9574},
  doi = {10.1111/j.1467-9574.1976.tb00275.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9574.1976.tb00275.x},
  urldate = {2021-06-18},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9574.1976.tb00275.x},
  file = {/home/tillb/Zotero/storage/7JNKHIPS/Haan - 1976 - Sample extremes an elementary introduction.pdf;/home/tillb/Zotero/storage/X4XJF53J/j.1467-9574.1976.tb00275.html},
  langid = {english},
  number = {4}
}

@online{heinWhyReLUNetworks2019,
  title = {Why {{ReLU}} Networks Yield High-Confidence Predictions Far Away from the Training Data and How to Mitigate the Problem},
  author = {Hein, Matthias and Andriushchenko, Maksym and Bitterwolf, Julian},
  date = {2019-05-07},
  url = {http://arxiv.org/abs/1812.05720},
  urldate = {2021-04-08},
  abstract = {Classifiers used in the wild, in particular for safety-critical systems, should not only have good generalization properties but also should know when they don't know, in particular make low confidence predictions far away from the training data. We show that ReLU type neural networks which yield a piecewise linear classifier function fail in this regard as they produce almost always high confidence predictions far away from the training data. For bounded domains like images we propose a new robust optimization technique similar to adversarial training which enforces low confidence predictions far away from the training data. We show that this technique is surprisingly effective in reducing the confidence of predictions far away from the training data while maintaining high confidence predictions and test error on the original classification task compared to standard training.},
  archiveprefix = {arXiv},
  eprint = {1812.05720},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/VT7LM65U/Hein et al. - 2019 - Why ReLU networks yield high-confidence prediction.pdf;/home/tillb/Zotero/storage/NC4RTTX8/1812.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{kellerLearningExtremalRepresentations2020,
  title = {Learning {{Extremal Representations}} with {{Deep Archetypal Analysis}}},
  author = {Keller, Sebastian Mathias and Samarin, Maxim and Arend Torres, Fabricio and Wieser, Mario and Roth, Volker},
  date = {2020-12-23},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  issn = {1573-1405},
  doi = {10.1007/s11263-020-01390-3},
  url = {https://doi.org/10.1007/s11263-020-01390-3},
  urldate = {2021-03-16},
  abstract = {Archetypes represent extreme manifestations of a population with respect to specific characteristic traits or features. In linear feature space, archetypes approximate the data convex hull allowing all data points to be expressed as convex mixtures of archetypes. As mixing of archetypes is performed directly on the input data, linear Archetypal Analysis requires additivity of the input, which is a strong assumption unlikely to hold e.g. in case of image data. To address this problem, we propose learning an appropriate latent feature space while simultaneously identifying suitable archetypes. We thus introduce a generative formulation of the linear archetype model, parameterized by neural networks. By introducing the distance-dependent archetype loss, the linear archetype model can be integrated into the latent space of a deep variational information bottleneck and an optimal representation, together with the archetypes, can be learned end-to-end. Moreover, the information bottleneck framework allows for a natural incorporation of arbitrarily complex side information during training. As a consequence, learned archetypes become easily interpretable as they derive their meaning directly from the included side information. Applicability of the proposed method is demonstrated by exploring archetypes of female facial expressions while using multi-rater based emotion scores of these expressions as side information. A second application illustrates the exploration of the chemical space of small organic molecules. By using different kinds of side information we demonstrate how identified archetypes, along with their interpretation, largely depend on the side information provided.},
  file = {/home/tillb/Zotero/storage/WKG2R3CG/Keller et al. - 2020 - Learning Extremal Representations with Deep Archet.pdf},
  langid = {english}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2021-05-22},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/YBQL9VB7/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/tillb/Zotero/storage/RV2PU6B5/1412.html},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@online{kingmaGlowGenerativeFlow2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  author = {Kingma, Diederik P. and Dhariwal, Prafulla},
  date = {2018-07-10},
  url = {http://arxiv.org/abs/1807.03039},
  urldate = {2021-05-22},
  abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
  archiveprefix = {arXiv},
  eprint = {1807.03039},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/2FLVB94C/Kingma and Dhariwal - 2018 - Glow Generative Flow with Invertible 1x1 Convolut.pdf;/home/tillb/Zotero/storage/K3JLQATZ/1807.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{kingmaIntroductionVariationalAutoencoders2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2019},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {FNT in Machine Learning},
  volume = {12},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  url = {http://arxiv.org/abs/1906.02691},
  urldate = {2021-05-22},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archiveprefix = {arXiv},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/4XDBV92P/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf;/home/tillb/Zotero/storage/MDIIHYM6/1906.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  number = {4}
}

@online{kirichenkoWhyNormalizingFlows2020,
  title = {Why {{Normalizing Flows Fail}} to {{Detect Out}}-of-{{Distribution Data}}},
  author = {Kirichenko, Polina and Izmailov, Pavel and Wilson, Andrew Gordon},
  date = {2020-06-15},
  url = {http://arxiv.org/abs/2006.08545},
  urldate = {2021-04-08},
  abstract = {Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic image-to-latent-space transformations which are not specific to the target image dataset. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection.},
  archiveprefix = {arXiv},
  eprint = {2006.08545},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/CER7QYYV/Kirichenko et al. - 2020 - Why Normalizing Flows Fail to Detect Out-of-Distri.pdf;/home/tillb/Zotero/storage/4HT22R7H/2006.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@book{knorrDistanceBasedOutliersAlgorithms2000,
  title = {Distance-{{Based Outliers}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Distance-{{Based Outliers}}},
  author = {Knorr, Edwin M. and Ng, Raymond T. and Tucakov, Vladimir},
  date = {2000},
  abstract = {. This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers can only deal efficiently with two dimensions/attributes of a dataset. In this paper, we study the notion of DB- (Distance- Based) outliers. Specifically, we show that: (i) outlier detection can be done efficiently for large datasets, and for k-dimensional datasets with large values of k (e.g., k  5); and (ii), outlier detection is a meaningful and important knowledge discovery task.  First, we present two simple algorithms, both having a complexity of O(kN  2  ), k being the dimensionality and N being the number of objects in the dataset. These algorithms readily support datasets with many more than two attributes. Second, we ...},
  file = {/home/tillb/Zotero/storage/I5YJTG44/Knorr et al. - 2000 - Distance-Based Outliers Algorithms and Applicatio.pdf;/home/tillb/Zotero/storage/QZCJD6ZC/summary.html}
}

@article{knorrDistancebasedOutliersAlgorithms2000,
  title = {Distance-Based Outliers: Algorithms and Applications},
  shorttitle = {Distance-Based Outliers},
  author = {Knorr, Edwin M. and Ng, Raymond T. and Tucakov, Vladimir},
  date = {2000-02-01},
  journaltitle = {The VLDB Journal},
  shortjournal = {The VLDB Journal},
  volume = {8},
  pages = {237--253},
  issn = {0949-877X},
  doi = {10.1007/s007780050006},
  url = {https://doi.org/10.1007/s007780050006},
  urldate = {2021-05-22},
  abstract = {This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers can only deal efficiently with two dimensions/attributes of a dataset. In this paper, we study the notion of DB (distance-based) outliers. Specifically, we show that (i) outlier detection can be done efficiently for large datasets, and for k-dimensional datasets with large values of k (e.g., \$k \textbackslash ge 5\$); and (ii), outlier detection is a meaningful and important knowledge discovery task.},
  file = {/home/tillb/Zotero/storage/4JWM4JGI/Knorr et al. - 2000 - Distance-based outliers algorithms and applicatio.pdf},
  langid = {english},
  number = {3}
}

@article{kobyzevNormalizingFlowsIntroduction2020,
  title = {Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of {{Current Methods}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  date = {2020},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2992934},
  url = {http://arxiv.org/abs/1908.09257},
  urldate = {2021-04-22},
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archiveprefix = {arXiv},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/YJSW2B78/Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf;/home/tillb/Zotero/storage/7VM2TQ7S/1908.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  url = {https://www.nature.com/articles/nature14539},
  urldate = {2021-05-22},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  file = {/home/tillb/Zotero/storage/CDZ8WKZY/LeCun et al. - 2015 - Deep learning.pdf;/home/tillb/Zotero/storage/6MWJZ5C8/nature14539.html},
  issue = {7553},
  langid = {english},
  number = {7553}
}

@online{leeTrainingConfidencecalibratedClassifiers2018,
  title = {Training {{Confidence}}-Calibrated {{Classifiers}} for {{Detecting Out}}-of-{{Distribution Samples}}},
  author = {Lee, Kimin and Lee, Honglak and Lee, Kibok and Shin, Jinwoo},
  date = {2018-02-23},
  url = {http://arxiv.org/abs/1711.09325},
  urldate = {2021-04-08},
  abstract = {The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.},
  archiveprefix = {arXiv},
  eprint = {1711.09325},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/P98DU5F4/Lee et al. - 2018 - Training Confidence-calibrated Classifiers for Det.pdf;/home/tillb/Zotero/storage/MFTYNDMH/1711.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{mackowiakGenerativeClassifiersBasis2020,
  title = {Generative {{Classifiers}} as a {{Basis}} for {{Trustworthy Image Classification}}},
  author = {Mackowiak, Radek and Ardizzone, Lynton and Köthe, Ullrich and Rother, Carsten},
  date = {2020-12-02},
  url = {http://arxiv.org/abs/2007.15036},
  urldate = {2021-04-08},
  abstract = {With the maturing of deep learning systems, trustworthiness is becoming increasingly important for model assessment. We understand trustworthiness as the combination of explainability and robustness. Generative classifiers (GCs) are a promising class of models that are said to naturally accomplish these qualities. However, this has mostly been demonstrated on simple datasets such as MNIST and CIFAR in the past. In this work, we firstly develop an architecture and training scheme that allows GCs to operate on a more relevant level of complexity for practical computer vision, namely the ImageNet challenge. Secondly, we demonstrate the immense potential of GCs for trustworthy image classification. Explainability and some aspects of robustness are vastly improved compared to feed-forward models, even when the GCs are just applied naively. While not all trustworthiness problems are solved completely, we observe that GCs are a highly promising basis for further algorithms and modifications. We release our trained model for download in the hope that it serves as a starting point for other generative classification tasks, in much the same way as pretrained ResNet architectures do for discriminative classification.},
  archiveprefix = {arXiv},
  eprint = {2007.15036},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/F2BDXLGL/Mackowiak et al. - 2020 - Generative Classifiers as a Basis for Trustworthy .pdf;/home/tillb/Zotero/storage/BYJPMALZ/2007.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@inproceedings{meinkeNeuralNetworksThat2019,
  title = {Towards Neural Networks That Provably Know When They Don't Know},
  author = {Meinke, Alexander and Hein, Matthias},
  date = {2019-09-25},
  url = {https://openreview.net/forum?id=ByxGkySKwH},
  urldate = {2021-04-08},
  abstract = {It has recently been shown that ReLU networks produce arbitrarily over-confident predictions far away from the  training data. Thus, ReLU networks do not know when they don't know. However, this is...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  file = {/home/tillb/Zotero/storage/TLJVDQIH/Meinke and Hein - 2019 - Towards neural networks that provably know when th.pdf;/home/tillb/Zotero/storage/S9RBPA2A/forum.html},
  langid = {english}
}

@online{nalisnickDetectingOutofDistributionInputs2019,
  title = {Detecting {{Out}}-of-{{Distribution Inputs}} to {{Deep Generative Models Using Typicality}}},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Lakshminarayanan, Balaji},
  date = {2019-10-16},
  url = {http://arxiv.org/abs/1906.02994},
  urldate = {2021-04-08},
  abstract = {Recent work has shown that deep generative models can assign higher likelihood to out-of-distribution data sets than to their training data (Nalisnick et al., 2019; Choi et al., 2019). We posit that this phenomenon is caused by a mismatch between the model's typical set and its areas of high probability density. In-distribution inputs should reside in the former but not necessarily in the latter, as previous work has presumed. To determine whether or not inputs reside in the typical set, we propose a statistically principled, easy-to-implement test using the empirical distribution of model likelihoods. The test is model agnostic and widely applicable, only requiring that the likelihood can be computed or closely approximated. We report experiments showing that our procedure can successfully detect the out-of-distribution sets in several of the challenging cases reported by Nalisnick et al. (2019).},
  archiveprefix = {arXiv},
  eprint = {1906.02994},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/ZGUV88RE/Nalisnick et al. - 2019 - Detecting Out-of-Distribution Inputs to Deep Gener.pdf;/home/tillb/Zotero/storage/VIIWPGJK/1906.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@incollection{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An}} Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché- Buc, F. and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  file = {/home/tillb/Zotero/storage/EWEZZ6N7/Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf},
  options = {useprefix=true}
}

@online{renLikelihoodRatiosOutofDistribution2019,
  title = {Likelihood {{Ratios}} for {{Out}}-of-{{Distribution Detection}}},
  author = {Ren, Jie and Liu, Peter J. and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and DePristo, Mark A. and Dillon, Joshua V. and Lakshminarayanan, Balaji},
  date = {2019-12-05},
  url = {http://arxiv.org/abs/1906.02845},
  urldate = {2021-04-08},
  abstract = {Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.},
  archiveprefix = {arXiv},
  eprint = {1906.02845},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/XMJK2HS5/Ren et al. - 2019 - Likelihood Ratios for Out-of-Distribution Detectio.pdf;/home/tillb/Zotero/storage/8W23SQI4/1906.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{rezendeVariationalInferenceNormalizing2016,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  date = {2016-06-14},
  url = {http://arxiv.org/abs/1505.05770},
  urldate = {2021-05-22},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archiveprefix = {arXiv},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/57VUZUFA/Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf;/home/tillb/Zotero/storage/SQC6BX7Z/1505.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  primaryclass = {cs, stat}
}

@online{schottFirstAdversariallyRobust2018,
  title = {Towards the First Adversarially Robust Neural Network Model on {{MNIST}}},
  author = {Schott, Lukas and Rauber, Jonas and Bethge, Matthias and Brendel, Wieland},
  date = {2018-09-20},
  url = {http://arxiv.org/abs/1805.09190},
  urldate = {2021-04-08},
  abstract = {Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful defense by Madry et al. (1) overfits on the L-infinity metric (it's highly susceptible to L2 and L0 perturbations), (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-infinity perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.},
  archiveprefix = {arXiv},
  eprint = {1805.09190},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/2D3RR6MA/Schott et al. - 2018 - Towards the first adversarially robust neural netw.pdf;/home/tillb/Zotero/storage/HTU4IF2E/1805.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs},
  version = {3}
}

@online{sorrensonDisentanglementNonlinearICA2020,
  title = {Disentanglement by {{Nonlinear ICA}} with {{General Incompressible}}-Flow {{Networks}} ({{GIN}})},
  author = {Sorrenson, Peter and Rother, Carsten and Köthe, Ullrich},
  date = {2020-01-14},
  url = {http://arxiv.org/abs/2001.04872},
  urldate = {2021-03-18},
  abstract = {A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.},
  archiveprefix = {arXiv},
  eprint = {2001.04872},
  eprinttype = {arxiv},
  file = {/home/tillb/Zotero/storage/7AXE4PDU/Sorrenson et al. - 2020 - Disentanglement by Nonlinear ICA with General Inco.pdf;/home/tillb/Zotero/storage/5YPA4MHW/2001.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@book{vershyninHighdimensionalProbabilityIntroduction2018,
  title = {High-Dimensional Probability: {{An}} Introduction with Applications in Data Science},
  author = {Vershynin, Roman},
  date = {2018},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/9781108231596},
  collection = {Cambridge Series in Statistical and Probabilistic Mathematics},
  file = {/home/tillb/Zotero/storage/49F2QRQA/Vershynin - High-Dimensional Probability.pdf},
  series = {Cambridge Series in Statistical and Probabilistic Mathematics}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

