
@inproceedings{abrolGeometricApproachArchetypal2020,
  title = {A {{Geometric Approach}} to {{Archetypal Analysis}} via {{Sparse Projections}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Abrol, Vinayak and Sharma, Pulkit},
  date = {2020-11-21},
  pages = {42--51},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v119/abrol20a.html},
  urldate = {2021-04-08},
  abstract = {Archetypal analysis (AA) aims to extract patterns using self-expressive decomposition of data as convex combinations of extremal points (on the convex hull) of the data. This work presents a comput...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/tillb/Zotero/storage/HEKQJ9YA/Abrol and Sharma - 2020 - A Geometric Approach to Archetypal Analysis via Sp.pdf;/home/tillb/Zotero/storage/D95EV9F2/abrol20a.html}
}

@incollection{anejaModelingStylizedCharacter2017a,
  title = {Modeling {{Stylized Character Expressions}} via {{Deep Learning}}},
  booktitle = {Computer {{Vision}} – {{ACCV}} 2016},
  author = {Aneja, Deepali and Colburn, Alex and Faigin, Gary and Shapiro, Linda and Mones, Barbara},
  editor = {Lai, Shang-Hong and Lepetit, Vincent and Nishino, Ko and Sato, Yoichi},
  date = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {10112},
  pages = {136--153},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-54184-6_9},
  url = {http://link.springer.com/10.1007/978-3-319-54184-6_9},
  urldate = {2021-04-19},
  abstract = {We propose DeepExpr, a novel expression transfer approach from humans to multiple stylized characters. We first train two Convolutional Neural Networks to recognize the expression of humans and stylized characters independently. Then we utilize a transfer learning technique to learn the mapping from humans to characters to create a shared embedding feature space. This embedding also allows human expression-based image retrieval and character expression-based image retrieval. We use our perceptual model to retrieve character expressions corresponding to humans. We evaluate our method on a set of retrieval tasks on our collected stylized character dataset of expressions. We also show that the ranking order predicted by the proposed features is highly correlated with the ranking order provided by a facial expression expert and Mechanical Turk experiments.},
  isbn = {978-3-319-54183-9 978-3-319-54184-6},
  langid = {english},
  file = {/home/tillb/Zotero/storage/E9J8GT9T/Aneja et al. - 2017 - Modeling Stylized Character Expressions via Deep L.pdf}
}

@inproceedings{ardizzoneAnalyzingInverseProblems2018,
  title = {Analyzing {{Inverse Problems}} with {{Invertible Neural Networks}}},
  author = {Ardizzone, Lynton and Kruse, Jakob and Rother, Carsten and Köthe, Ullrich},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=rJed6j0cKX},
  urldate = {2021-03-16},
  abstract = {To analyze inverse problems with Invertible Neural Networks},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/tillb/Zotero/storage/WRAK38WS/Ardizzone et al. - 2018 - Analyzing Inverse Problems with Invertible Neural .pdf;/home/tillb/Zotero/storage/FCGI9BG9/forum.html}
}

@article{bauckhageArchetypalAnalysisAutoencoder2015,
  title = {Archetypal {{Analysis}} as an {{Autoencoder}}},
  author = {Bauckhage, C. and Kersting, K. and Hoppe, F. and Thurau, C.},
  date = {2015},
  journaltitle = {Workshop New Challenges in Neural Computation 2015},
  pages = {8--16},
  langid = {english},
  file = {/home/tillb/Zotero/storage/LW24EEIC/Hammer et al. - 2015 - Workshop New Challenges in Neural Computation 2015.pdf}
}

@inproceedings{baumSupervisedLearningProbability1988,
  title = {Supervised {{Learning}} of {{Probability Distributions}} by {{Neural Networks}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Baum, Eric and Wilczek, Frank},
  editor = {Anderson, D.},
  date = {1988},
  publisher = {{American Institute of Physics}},
  url = {https://proceedings.neurips.cc/paper/1987/file/eccbc87e4b5ce2fe28308fd9f2a7baf3-Paper.pdf},
  urldate = {2021-05-22},
  file = {/home/tillb/Zotero/storage/TWHK7AYC/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html}
}

@online{clanuwatDeepLearningClassical2018,
  title = {Deep Learning for Classical Japanese Literature},
  author = {Clanuwat, Tarin and Bober-Irizar, Mikel and Kitamoto, Asanobu and Lamb, Alex and Yamamoto, Kazuaki and Ha, David},
  year = {2018-12-03, 2018},
  eprint = {1812.01718},
  eprinttype = {arxiv},
  primaryclass = {cs.CV},
  archiveprefix = {arXiv}
}

@online{cohenEMNISTExtensionMNIST2017,
  title = {{{EMNIST}}: An Extension of {{MNIST}} to Handwritten Letters},
  shorttitle = {{{EMNIST}}},
  author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, André},
  options = {useprefix=true},
  date = {2017-03-01},
  eprint = {1702.05373},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1702.05373},
  urldate = {2021-04-19},
  abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/tillb/Zotero/storage/C24TW97H/Cohen et al. - 2017 - EMNIST an extension of MNIST to handwritten lette.pdf;/home/tillb/Zotero/storage/6ZC46HCJ/1702.html}
}

@book{colesIntroductionStatisticalModeling2001,
  title = {An {{Introduction}} to {{Statistical Modeling}} of {{Extreme Values}}},
  author = {Coles, Stuart},
  date = {2001},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  location = {{London}},
  doi = {10.1007/978-1-4471-3675-0},
  url = {https://www.springer.com/de/book/9781852334598},
  urldate = {2021-06-18},
  abstract = {Directly oriented towards real practical application, this book develops both the basic theoretical framework of extreme value models and the statistical inferential techniques for using these models in practice. Intended for statisticians and non-statisticians alike, the theoretical treatment is elementary, with heuristics often replacing detailed mathematical proof. Most aspects of extreme modeling techniques are covered, including historical techniques (still widely used) and contemporary techniques based on point process models. A wide range of worked examples, using genuine datasets, illustrate the various modeling procedures and a concluding chapter provides a brief introduction to a number of more advanced topics, including Bayesian inference and spatial extremes. All the computations are carried out using S-PLUS, and the corresponding datasets and functions are available via the Internet for readers to recreate examples for themselves. An essential reference for students and researchers in statistics and disciplines such as engineering, finance and environmental science, this book will also appeal to practitioners looking for practical help in solving real problems. Stuart Coles is Reader in Statistics at the University of Bristol, UK, having previously lectured at the universities of Nottingham and Lancaster. In 1992 he was the first recipient of the Royal Statistical Society's research prize. He has published widely in the statistical literature, principally in the area of extreme value modeling.},
  isbn = {978-1-85233-459-8},
  langid = {english},
  file = {/home/tillb/Zotero/storage/NGJBGD2M/Coles - 2001 - An Introduction to Statistical Modeling of Extreme.pdf;/home/tillb/Zotero/storage/48DBL9QN/9781852334598.html}
}

@article{cutlerArchetypalAnalysis1994,
  title = {Archetypal {{Analysis}}},
  author = {Cutler, Adele and Breiman, Leo},
  date = {1994-11-01},
  journaltitle = {Technometrics},
  volume = {36},
  number = {4},
  pages = {338--347},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1994.10485840},
  url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1994.10485840},
  urldate = {2021-06-21},
  abstract = {Archetypal analysis represents each individual in a data set as a mixture of individuals of pure type or archetypes. The archetypes themselves are restricted to being mixtures of the individuals in the data set. Archetypes are selected by minimizing the squared error in representing each individual as a mixture of archetypes. The usefulness of archetypal analysis is illustrated on several data sets. Computing the archetypes is a nonlinear least squares problem, which is solved using an alternating minimizing algorithm.},
  keywords = {Archetypes,Convex hull,Graphics,Nonlinear optimization,Principal components},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1994.10485840},
  file = {/home/tillb/Zotero/storage/PW4MARU3/Cutler and Breiman - 1994 - Archetypal Analysis.pdf;/home/tillb/Zotero/storage/GW2N7DJ8/00401706.1994.html}
}

@online{dalalAutoregressiveModelsWhat2019,
  title = {Autoregressive {{Models}}: {{What Are They Good For}}?},
  shorttitle = {Autoregressive {{Models}}},
  author = {Dalal, Murtaza and Li, Alexander C. and Taori, Rohan},
  date = {2019-10-17},
  eprint = {1910.07737},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.07737},
  urldate = {2021-04-08},
  abstract = {Autoregressive (AR) models have become a popular tool for unsupervised learning, achieving state-of-the-art log likelihood estimates. We investigate the use of AR models as density estimators in two settings -- as a learning signal for image translation, and as an outlier detector -- and find that these density estimates are much less reliable than previously thought. We examine the underlying optimization issues from both an empirical and theoretical perspective, and provide a toy example that illustrates the problem. Overwhelmingly, we find that density estimates do not correlate with perceptual quality and are unhelpful for downstream tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/EHTDCS9B/Dalal et al. - 2019 - Autoregressive Models What Are They Good For.pdf;/home/tillb/Zotero/storage/Y4LFKTDC/1910.html}
}

@article{dehaanSampleExtremesElementary1976,
  title = {Sample Extremes: An Elementary Introduction},
  shorttitle = {Sample Extremes},
  author = {de Haan, Laurens},
  options = {useprefix=true},
  date = {1976},
  journaltitle = {Statistica Neerlandica},
  volume = {30},
  number = {4},
  pages = {161--172},
  issn = {1467-9574},
  doi = {10.1111/j.1467-9574.1976.tb00275.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9574.1976.tb00275.x},
  urldate = {2021-06-18},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9574.1976.tb00275.x},
  file = {/home/tillb/Zotero/storage/7JNKHIPS/Haan - 1976 - Sample extremes an elementary introduction.pdf;/home/tillb/Zotero/storage/X4XJF53J/j.1467-9574.1976.tb00275.html}
}

@online{dinhDensityEstimationUsing2017,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  date = {2017-02-27},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1605.08803},
  urldate = {2021-04-04},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/DKVDJU8L/Dinh et al. - 2017 - Density estimation using Real NVP.pdf;/home/tillb/Zotero/storage/R8FRYFNP/1605.html}
}

@online{dinhNICENonlinearIndependent2015,
  title = {{{NICE}}: {{Non-linear Independent Components Estimation}}},
  shorttitle = {{{NICE}}},
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  date = {2015-04-10},
  eprint = {1410.8516},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1410.8516},
  urldate = {2021-05-22},
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/tillb/Zotero/storage/RV47GJZD/Dinh et al. - 2015 - NICE Non-linear Independent Components Estimation.pdf;/home/tillb/Zotero/storage/M58U99EF/1410.html}
}

@online{europeanmathematicalsocietyOutlierEncyclopediaMathematics,
  title = {Outlier - {{Encyclopedia}} of {{Mathematics}}},
  author = {{European Mathematical Society}},
  url = {https://encyclopediaofmath.org/index.php?title=Outlier},
  urldate = {2021-07-05},
  file = {/home/tillb/Zotero/storage/W399IYD3/index.html}
}

@incollection{forbesChiSquaredDistribution2010,
  title = {Chi-{{Squared Distribution}}},
  booktitle = {Statistical {{Distributions}}},
  author = {Forbes, Catherine and Evans, Merran and Hastings, Nicholas and Peacock, Brian},
  date = {2010},
  pages = {69--73},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470627242.ch11},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470627242.ch11},
  urldate = {2021-06-18},
  abstract = {Important applications of the chi-squared variate arise from the fact that it is the distribution of the sum of the squares of a number of normal variates. Where a set of data is represented by a theoretical model, the chi-squared distribution can be used to test the goodness of fit between the observed data points and the values predicted by the model, subject to the differences being normally distributed. A particularly common application is the analysis of contingency tables. This chapter discusses variate relationships, random number generation and chi distribution. Controlled Vocabulary Terms chi-squared distribution; control variate},
  isbn = {978-0-470-62724-2},
  langid = {english},
  keywords = {chi-squared distribution,random number,variate},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470627242.ch11},
  file = {/home/tillb/Zotero/storage/MRGQUE62/Forbes et al. - 2010 - Chi-Squared Distribution.pdf;/home/tillb/Zotero/storage/SQMKV25P/9780470627242.html}
}

@incollection{forbesDirichletDistribution2010,
  title = {Dirichlet {{Distribution}}},
  booktitle = {Statistical {{Distributions}}},
  author = {Forbes, Catherine and Evans, Merran and Hastings, Nicholas and Peacock, Brian},
  date = {2010},
  pages = {77--78},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470627242.ch13},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470627242.ch13},
  urldate = {2021-06-18},
  abstract = {The standard or Type I Dirichlet is a multivariate generalization of the beta distribution. This chapter discusses variate relationships of Dirichlet variate. The Dirichlet multinominal distribution is the multivariate generalization of the beta binomial distribution. It is also known as the compound multinomial distribution and, for integer parameters, the multivariate negative hypergeometric distribution. Controlled Vocabulary Terms beta distribution; binomial distribution; control variate; Dirichlet distribution; multivariate statistics},
  isbn = {978-0-470-62724-2},
  langid = {english},
  keywords = {chi-squared distribution,random number,variate},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470627242.ch13},
  file = {/home/tillb/Zotero/storage/MRGQUE62/Forbes et al. - 2010 - Chi-Squared Distribution.pdf}
}

@online{gomezReversibleResidualNetwork2017,
  title = {The {{Reversible Residual Network}}: {{Backpropagation Without Storing Activations}}},
  shorttitle = {The {{Reversible Residual Network}}},
  author = {Gomez, Aidan N. and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B.},
  date = {2017-07-13},
  eprint = {1707.04585},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1707.04585},
  urldate = {2021-05-28},
  abstract = {Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/tillb/Zotero/storage/PQQ5P4ZQ/Gomez et al. - 2017 - The Reversible Residual Network Backpropagation W.pdf;/home/tillb/Zotero/storage/WIJUTPK2/1707.html}
}

@book{goochEncyclopedicDictionaryPolymers2011,
  title = {Encyclopedic {{Dictionary}} of {{Polymers}}},
  editor = {Gooch, Jan W.},
  date = {2011},
  edition = {2},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  url = {https://www.springer.com/gp/book/9781441962461},
  urldate = {2021-06-18},
  abstract = {This reference, in its second edition, contains more than 7,500 polymeric material terms, including the names of chemicals, processes, formulae, and analytical methods that are used frequently in the polymer and engineering fields. In view of the evolving partnership between physical and life sciences, this title includes an appendix of biochemical and microbiological terms (thus offering previously unpublished material, distinct from all competitors.) Each succinct entry offers a broadly accessible definition as well as cross-references to related terms. Where appropriate to enhance clarity further, the volume's definitions may also offer equations, chemical structures, and other figures. The new interactive software facilitates easy access to a large database of chemical structures (2D/3D-view), audio files for pronunciation, polymer science equations and many more.},
  isbn = {978-1-4419-6246-1},
  langid = {english},
  file = {/home/tillb/Zotero/storage/KLBRZKVA/9781441962461.html}
}

@inproceedings{goodfellowGenerativeAdversarialNets2014,
  title = {Generative Adversarial Nets},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-12-08},
  series = {{{NIPS}}'14},
  pages = {2672--2680},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  file = {/home/tillb/Zotero/storage/8DZVUSLT/Goodfellow et al. - Generative Adversarial Nets.pdf}
}

@online{heinWhyReLUNetworks2019,
  title = {Why {{ReLU}} Networks Yield High-Confidence Predictions Far Away from the Training Data and How to Mitigate the Problem},
  author = {Hein, Matthias and Andriushchenko, Maksym and Bitterwolf, Julian},
  date = {2019-05-07},
  eprint = {1812.05720},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.05720},
  urldate = {2021-04-08},
  abstract = {Classifiers used in the wild, in particular for safety-critical systems, should not only have good generalization properties but also should know when they don't know, in particular make low confidence predictions far away from the training data. We show that ReLU type neural networks which yield a piecewise linear classifier function fail in this regard as they produce almost always high confidence predictions far away from the training data. For bounded domains like images we propose a new robust optimization technique similar to adversarial training which enforces low confidence predictions far away from the training data. We show that this technique is surprisingly effective in reducing the confidence of predictions far away from the training data while maintaining high confidence predictions and test error on the original classification task compared to standard training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/VT7LM65U/Hein et al. - 2019 - Why ReLU networks yield high-confidence prediction.pdf;/home/tillb/Zotero/storage/NC4RTTX8/1812.html}
}

@article{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015-02-11},
  url = {https://arxiv.org/abs/1502.03167v3},
  urldate = {2021-07-07},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  langid = {english},
  file = {/home/tillb/Zotero/storage/6QH9ARF5/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf;/home/tillb/Zotero/storage/NBXMFGK5/1502.html}
}

@article{jamesGeneralisedInverse1978,
  title = {The Generalised Inverse},
  author = {James, M.},
  date = {1978-06},
  journaltitle = {The Mathematical Gazette},
  volume = {62},
  number = {420},
  pages = {109--114},
  publisher = {{Cambridge University Press}},
  issn = {0025-5572, 2056-6328},
  doi = {10.1017/S0025557200086460},
  url = {https://www.cambridge.org/core/journals/mathematical-gazette/article/abs/generalised-inverse/8B58A1EC63AD8C38BE2AB74A5175028F},
  urldate = {2021-07-06},
  abstract = {If we have a system of m linear equations to solve, it is a great simplification to write them in matrix form                                                Ax = b                ,             where A is an m x n matrix of coefficients, b is an m-dimensional vector of constants and x is an n-dimensional vector of unknowns.},
  langid = {english},
  file = {/home/tillb/Zotero/storage/KG5JDHZP/James - 1978 - The generalised inverse.pdf;/home/tillb/Zotero/storage/DP2PVAR2/8B58A1EC63AD8C38BE2AB74A5175028F.html}
}

@article{kellerLearningExtremalRepresentations2020,
  title = {Learning {{Extremal Representations}} with {{Deep Archetypal Analysis}}},
  author = {Keller, Sebastian Mathias and Samarin, Maxim and Arend Torres, Fabricio and Wieser, Mario and Roth, Volker},
  date = {2020-12-23},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  issn = {1573-1405},
  doi = {10.1007/s11263-020-01390-3},
  url = {https://doi.org/10.1007/s11263-020-01390-3},
  urldate = {2021-03-16},
  abstract = {Archetypes represent extreme manifestations of a population with respect to specific characteristic traits or features. In linear feature space, archetypes approximate the data convex hull allowing all data points to be expressed as convex mixtures of archetypes. As mixing of archetypes is performed directly on the input data, linear Archetypal Analysis requires additivity of the input, which is a strong assumption unlikely to hold e.g. in case of image data. To address this problem, we propose learning an appropriate latent feature space while simultaneously identifying suitable archetypes. We thus introduce a generative formulation of the linear archetype model, parameterized by neural networks. By introducing the distance-dependent archetype loss, the linear archetype model can be integrated into the latent space of a deep variational information bottleneck and an optimal representation, together with the archetypes, can be learned end-to-end. Moreover, the information bottleneck framework allows for a natural incorporation of arbitrarily complex side information during training. As a consequence, learned archetypes become easily interpretable as they derive their meaning directly from the included side information. Applicability of the proposed method is demonstrated by exploring archetypes of female facial expressions while using multi-rater based emotion scores of these expressions as side information. A second application illustrates the exploration of the chemical space of small organic molecules. By using different kinds of side information we demonstrate how identified archetypes, along with their interpretation, largely depend on the side information provided.},
  langid = {english},
  file = {/home/tillb/Zotero/storage/WKG2R3CG/Keller et al. - 2020 - Learning Extremal Representations with Deep Archet.pdf}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2021-05-22},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/tillb/Zotero/storage/YBQL9VB7/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/tillb/Zotero/storage/RV2PU6B5/1412.html}
}

@online{kingmaGlowGenerativeFlow2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  author = {Kingma, Diederik P. and Dhariwal, Prafulla},
  date = {2018-07-10},
  eprint = {1807.03039},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.03039},
  urldate = {2021-05-22},
  abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/2FLVB94C/Kingma and Dhariwal - 2018 - Glow Generative Flow with Invertible 1x1 Convolut.pdf;/home/tillb/Zotero/storage/K3JLQATZ/1807.html}
}

@article{kingmaIntroductionVariationalAutoencoders2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2019},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {FNT in Machine Learning},
  volume = {12},
  number = {4},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  url = {http://arxiv.org/abs/1906.02691},
  urldate = {2021-05-22},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/4XDBV92P/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf;/home/tillb/Zotero/storage/MDIIHYM6/1906.html}
}

@online{kirichenkoWhyNormalizingFlows2020,
  title = {Why {{Normalizing Flows Fail}} to {{Detect Out-of-Distribution Data}}},
  author = {Kirichenko, Polina and Izmailov, Pavel and Wilson, Andrew Gordon},
  date = {2020-06-15},
  eprint = {2006.08545},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.08545},
  urldate = {2021-04-08},
  abstract = {Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic image-to-latent-space transformations which are not specific to the target image dataset. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/CER7QYYV/Kirichenko et al. - 2020 - Why Normalizing Flows Fail to Detect Out-of-Distri.pdf;/home/tillb/Zotero/storage/4HT22R7H/2006.html}
}

@book{knorrDistanceBasedOutliersAlgorithms2000,
  title = {Distance-{{Based Outliers}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Distance-{{Based Outliers}}},
  author = {Knorr, Edwin M. and Ng, Raymond T. and Tucakov, Vladimir},
  date = {2000},
  abstract = {. This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers can only deal efficiently with two dimensions/attributes of a dataset. In this paper, we study the notion of DB- (Distance- Based) outliers. Specifically, we show that: (i) outlier detection can be done efficiently for large datasets, and for k-dimensional datasets with large values of k (e.g., k  5); and (ii), outlier detection is a meaningful and important knowledge discovery task.  First, we present two simple algorithms, both having a complexity of O(kN  2  ), k being the dimensionality and N being the number of objects in the dataset. These algorithms readily support datasets with many more than two attributes. Second, we ...},
  file = {/home/tillb/Zotero/storage/I5YJTG44/Knorr et al. - 2000 - Distance-Based Outliers Algorithms and Applicatio.pdf;/home/tillb/Zotero/storage/QZCJD6ZC/summary.html}
}

@article{knorrDistancebasedOutliersAlgorithms2000,
  title = {Distance-Based Outliers: Algorithms and Applications},
  shorttitle = {Distance-Based Outliers},
  author = {Knorr, Edwin M. and Ng, Raymond T. and Tucakov, Vladimir},
  date = {2000-02-01},
  journaltitle = {The VLDB Journal},
  shortjournal = {The VLDB Journal},
  volume = {8},
  number = {3},
  pages = {237--253},
  issn = {0949-877X},
  doi = {10.1007/s007780050006},
  url = {https://doi.org/10.1007/s007780050006},
  urldate = {2021-05-22},
  abstract = {This paper deals with finding outliers (exceptions) in large, multidimensional datasets. The identification of outliers can lead to the discovery of truly unexpected knowledge in areas such as electronic commerce, credit card fraud, and even the analysis of performance statistics of professional athletes. Existing methods that we have seen for finding outliers can only deal efficiently with two dimensions/attributes of a dataset. In this paper, we study the notion of DB (distance-based) outliers. Specifically, we show that (i) outlier detection can be done efficiently for large datasets, and for k-dimensional datasets with large values of k (e.g., \$k \textbackslash ge 5\$); and (ii), outlier detection is a meaningful and important knowledge discovery task.},
  langid = {english},
  file = {/home/tillb/Zotero/storage/4JWM4JGI/Knorr et al. - 2000 - Distance-based outliers algorithms and applicatio.pdf}
}

@article{kobyzevNormalizingFlowsIntroduction2020,
  title = {Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of {{Current Methods}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  date = {2020},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2992934},
  url = {http://arxiv.org/abs/1908.09257},
  urldate = {2021-04-22},
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/YJSW2B78/Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf;/home/tillb/Zotero/storage/7VM2TQ7S/1908.html}
}

@report{krizhevskyLearningMultipleLayers2009,
  title = {Learning Multiple Layers of Features from Tiny Images},
  author = {Krizhevsky, Alex},
  date = {2009},
  abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly},
  file = {/home/tillb/Zotero/storage/A59MCYPF/Krizhevsky - 2009 - Learning multiple layers of features from tiny ima.pdf;/home/tillb/Zotero/storage/CH5I3JZ9/summary\;jsessionid=C2B3F1284A66F207921866BFF54C02C2.html}
}

@incollection{leadbetterAsymptoticDistributionsExtremes1983,
  title = {Asymptotic {{Distributions}} of {{Extremes}}},
  booktitle = {Extremes and {{Related Properties}} of {{Random Sequences}} and {{Processes}}},
  author = {Leadbetter, M. R. and Lindgren, Georg and Rootzén, Holger},
  editor = {Leadbetter, M. R. and Lindgren, Georg and Rootzén, Holger},
  date = {1983},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {3--30},
  publisher = {{Springer}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4612-5449-2_1},
  url = {https://doi.org/10.1007/978-1-4612-5449-2_1},
  urldate = {2021-06-28},
  abstract = {This chapter is primarily concerned with the central result of classical extreme value theory—the Extremal Types Theorem—which specifies the possible forms for the limiting distribution of maxima in sequences of i.i.d. random variables. In the derivation, the possible limiting distributions are identified with a class having a certain stability property—the so-called max-stable distributions. It is further shown that this class consists precisely of the three families known (loosely) as the three extreme value distributions.},
  isbn = {978-1-4612-5449-2},
  langid = {english},
  file = {/home/tillb/Zotero/storage/VFGBPLWP/Leadbetter et al. - 1983 - Asymptotic Distributions of Extremes.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  url = {https://www.nature.com/articles/nature14539},
  urldate = {2021-05-22},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  issue = {7553},
  langid = {english},
  file = {/home/tillb/Zotero/storage/CDZ8WKZY/LeCun et al. - 2015 - Deep learning.pdf;/home/tillb/Zotero/storage/6MWJZ5C8/nature14539.html}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/home/tillb/Zotero/storage/LQWB4E9H/726791.html}
}

@online{leeTrainingConfidencecalibratedClassifiers2018,
  title = {Training {{Confidence-calibrated Classifiers}} for {{Detecting Out-of-Distribution Samples}}},
  author = {Lee, Kimin and Lee, Honglak and Lee, Kibok and Shin, Jinwoo},
  date = {2018-02-23},
  eprint = {1711.09325},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1711.09325},
  urldate = {2021-04-08},
  abstract = {The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/P98DU5F4/Lee et al. - 2018 - Training Confidence-calibrated Classifiers for Det.pdf;/home/tillb/Zotero/storage/MFTYNDMH/1711.html}
}

@online{mackowiakGenerativeClassifiersBasis2020,
  title = {Generative {{Classifiers}} as a {{Basis}} for {{Trustworthy Image Classification}}},
  author = {Mackowiak, Radek and Ardizzone, Lynton and Köthe, Ullrich and Rother, Carsten},
  date = {2020-12-02},
  eprint = {2007.15036},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2007.15036},
  urldate = {2021-04-08},
  abstract = {With the maturing of deep learning systems, trustworthiness is becoming increasingly important for model assessment. We understand trustworthiness as the combination of explainability and robustness. Generative classifiers (GCs) are a promising class of models that are said to naturally accomplish these qualities. However, this has mostly been demonstrated on simple datasets such as MNIST and CIFAR in the past. In this work, we firstly develop an architecture and training scheme that allows GCs to operate on a more relevant level of complexity for practical computer vision, namely the ImageNet challenge. Secondly, we demonstrate the immense potential of GCs for trustworthy image classification. Explainability and some aspects of robustness are vastly improved compared to feed-forward models, even when the GCs are just applied naively. While not all trustworthiness problems are solved completely, we observe that GCs are a highly promising basis for further algorithms and modifications. We release our trained model for download in the hope that it serves as a starting point for other generative classification tasks, in much the same way as pretrained ResNet architectures do for discriminative classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/tillb/Zotero/storage/F2BDXLGL/Mackowiak et al. - 2020 - Generative Classifiers as a Basis for Trustworthy .pdf;/home/tillb/Zotero/storage/BYJPMALZ/2007.html}
}

@inproceedings{meinkeNeuralNetworksThat2019,
  title = {Towards Neural Networks That Provably Know When They Don't Know},
  author = {Meinke, Alexander and Hein, Matthias},
  date = {2019-09-25},
  url = {https://openreview.net/forum?id=ByxGkySKwH},
  urldate = {2021-04-08},
  abstract = {It has recently been shown that ReLU networks produce arbitrarily over-confident predictions far away from the  training data. Thus, ReLU networks do not know when they don't know. However, this is...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/tillb/Zotero/storage/TLJVDQIH/Meinke and Hein - 2019 - Towards neural networks that provably know when th.pdf;/home/tillb/Zotero/storage/S9RBPA2A/forum.html}
}

@online{mirzaConditionalGenerativeAdversarial2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  date = {2014-11-06},
  eprint = {1411.1784},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1411.1784},
  urldate = {2021-07-07},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/4BU8RDLX/Mirza and Osindero - 2014 - Conditional Generative Adversarial Nets.pdf;/home/tillb/Zotero/storage/WKB9ZWYA/1411.html}
}

@online{nalisnickDetectingOutofDistributionInputs2019,
  title = {Detecting {{Out-of-Distribution Inputs}} to {{Deep Generative Models Using Typicality}}},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Lakshminarayanan, Balaji},
  date = {2019-10-16},
  eprint = {1906.02994},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.02994},
  urldate = {2021-04-08},
  abstract = {Recent work has shown that deep generative models can assign higher likelihood to out-of-distribution data sets than to their training data (Nalisnick et al., 2019; Choi et al., 2019). We posit that this phenomenon is caused by a mismatch between the model's typical set and its areas of high probability density. In-distribution inputs should reside in the former but not necessarily in the latter, as previous work has presumed. To determine whether or not inputs reside in the typical set, we propose a statistically principled, easy-to-implement test using the empirical distribution of model likelihoods. The test is model agnostic and widely applicable, only requiring that the likelihood can be computed or closely approximated. We report experiments showing that our procedure can successfully detect the out-of-distribution sets in several of the challenging cases reported by Nalisnick et al. (2019).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/ZGUV88RE/Nalisnick et al. - 2019 - Detecting Out-of-Distribution Inputs to Deep Gener.pdf;/home/tillb/Zotero/storage/VIIWPGJK/1906.html}
}

@incollection{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An}} Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché- Buc, F. and Fox, E. and Garnett, R.},
  options = {useprefix=true},
  date = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  file = {/home/tillb/Zotero/storage/EWEZZ6N7/Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf}
}

@online{radfordUnsupervisedRepresentationLearning2016,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2016-01-07},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1511.06434},
  urldate = {2021-07-07},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/tillb/Zotero/storage/FHIFPZUP/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf;/home/tillb/Zotero/storage/VHIR4WPL/1511.html}
}

@online{renLikelihoodRatiosOutofDistribution2019,
  title = {Likelihood {{Ratios}} for {{Out-of-Distribution Detection}}},
  author = {Ren, Jie and Liu, Peter J. and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and DePristo, Mark A. and Dillon, Joshua V. and Lakshminarayanan, Balaji},
  date = {2019-12-05},
  eprint = {1906.02845},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.02845},
  urldate = {2021-04-08},
  abstract = {Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/XMJK2HS5/Ren et al. - 2019 - Likelihood Ratios for Out-of-Distribution Detectio.pdf;/home/tillb/Zotero/storage/8W23SQI4/1906.html}
}

@online{rezendeVariationalInferenceNormalizing2016,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  date = {2016-06-14},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1505.05770},
  urldate = {2021-05-22},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/tillb/Zotero/storage/57VUZUFA/Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf;/home/tillb/Zotero/storage/SQC6BX7Z/1505.html}
}

@online{schottFirstAdversariallyRobust2018,
  title = {Towards the First Adversarially Robust Neural Network Model on {{MNIST}}},
  author = {Schott, Lukas and Rauber, Jonas and Bethge, Matthias and Brendel, Wieland},
  date = {2018-09-20},
  eprint = {1805.09190},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1805.09190},
  urldate = {2021-04-08},
  abstract = {Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful defense by Madry et al. (1) overfits on the L-infinity metric (it's highly susceptible to L2 and L0 perturbations), (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-infinity perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.},
  archiveprefix = {arXiv},
  version = {3},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/tillb/Zotero/storage/2D3RR6MA/Schott et al. - 2018 - Towards the first adversarially robust neural netw.pdf;/home/tillb/Zotero/storage/HTU4IF2E/1805.html}
}

@online{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015-04-10},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2021-07-07},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/tillb/Zotero/storage/56XMVNGG/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;/home/tillb/Zotero/storage/VLI3K6AG/1409.html}
}

@online{sorrensonDisentanglementNonlinearICA2020,
  title = {Disentanglement by {{Nonlinear ICA}} with {{General Incompressible-flow Networks}} ({{GIN}})},
  author = {Sorrenson, Peter and Rother, Carsten and Köthe, Ullrich},
  date = {2020-01-14},
  eprint = {2001.04872},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2001.04872},
  urldate = {2021-03-18},
  abstract = {A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/tillb/Zotero/storage/7AXE4PDU/Sorrenson et al. - 2020 - Disentanglement by Nonlinear ICA with General Inco.pdf;/home/tillb/Zotero/storage/5YPA4MHW/2001.html}
}

@book{vershyninHighdimensionalProbabilityIntroduction2018,
  title = {High-Dimensional Probability: {{An}} Introduction with Applications in Data Science},
  author = {Vershynin, Roman},
  date = {2018},
  series = {Cambridge Series in Statistical and Probabilistic Mathematics},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/9781108231596},
  collection = {Cambridge Series in Statistical and Probabilistic Mathematics},
  file = {/home/tillb/Zotero/storage/49F2QRQA/Vershynin - High-Dimensional Probability.pdf}
}

@online{xiaoFashionmnistNovelImage2017,
  title = {Fashion-Mnist: A Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year = {2017-08-28, 2017},
  eprint = {1708.07747},
  eprinttype = {arxiv},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv}
}

@article{zimekThereBackAgain2018,
  title = {There and Back Again: {{Outlier}} Detection between Statistical Reasoning and Data Mining Algorithms},
  shorttitle = {There and Back Again},
  author = {Zimek, Arthur and Filzmoser, Peter},
  date = {2018},
  journaltitle = {WIREs Data Mining and Knowledge Discovery},
  volume = {8},
  number = {6},
  pages = {e1280},
  issn = {1942-4795},
  doi = {10.1002/widm.1280},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1280},
  urldate = {2021-07-08},
  abstract = {Outlier detection has been a topic in statistics for centuries. Over mainly the last two decades, there has been also an increasing interest in the database and data mining community to develop scalable methods for outlier detection. Initially based on statistical reasoning, however, these methods soon lost the direct probabilistic interpretability of the derived outlier scores. Here, we detail from a joint point of view of data mining and statistics the roots and the path of development of statistical outlier detection and of database-related data mining methods for outlier detection. We discuss their inherent meaning, review approaches to again find a statistically meaningful interpretation of outlier scores, and sketch related current research topics. This article is categorized under: Algorithmic Development {$>$} Statistics Algorithmic Development {$>$} Scalable Statistical Methods Technologies {$>$} Machine Learning},
  langid = {english},
  keywords = {anomaly detection,outlier detection,outlier model,statistics and data mining},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1280},
  file = {/home/tillb/Zotero/storage/Z3H4CC4R/Zimek and Filzmoser - 2018 - There and back again Outlier detection between st.pdf;/home/tillb/Zotero/storage/QG9G4CA7/widm.html}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

